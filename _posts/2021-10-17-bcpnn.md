---
title: "Pharmacovigilance Methods: BCPNNs"
layout: post
---

{% include mathjax-cdn.html %}
{% include details-dropdown.html %}

Pharmacovigilance is the field of science that monitors, detects, and prevents adverse drug events. While monitoring and prevention are critical components in drug safety, we will focus on signal detection. Signal detection in drug surveillance is primarily an analytics problem that needs to account for the complexities in healthcare data and spontaneous reporting systems.

In 1998, the Bayesian Confidence Propagation Neural Network (BCPNN) method was introduced to the field in a publication by Bates and team [1]. Their method is inspired by information theory and neural networks. However, the final method evokes a hypothesis testing framework as it applies Bayesian inference to detect signals (i.e., unexpectedly high observations) in the data.

An overview of the method is presented below. You can jump from section to section:

- [The Idea](#the-idea)
- [The Theory](#the-theory)
- [The Neural Network](#the-neural-network)
- [The Closed Form Solution(s)](#the-closed-form-solutions)
- [Citations](#citations)

## The Idea

The idea revolves around inferring the probability of an adverse drug event (A) given some drug of interest (D). To solve for this probability, $P(A\vert D)$, the researchers make the following assumptions:

1. **Prior Belief of Independence:** There are thousands of potential adverse events for any drug, and many of those events (if not all) are not associated with taking specific drugs. Therefore, the team asserts a prior assumption that $P(A)\perp\kern-5pt\perp P(D)$.
2. **Binomially-Distributed Data:** With respect to the observed data (e.g., spontaneous reporting system), the team asserts that the counts of (i) the drug of interest, (ii) the adverse event of interest, and (iii) the drug of interest and adverse event of interest co-occurring all follow a Binomial distribution.
    - Rejecting this assumption and asserting a Poisson distribution instead leads to an alternative model, the Gamma Poisson Shrinker (GPS) proposed by DuMouchel and Pregibon in 2011 [2].

The BCPNN method tests the initial assumption of independence through statistical inference.

## The Theory

As a quick summary of the 1998 paper [1], the team applies Bayes' law:

$$P(A\vert D) = \frac{P(A, D)}{P(D)} = P(A)\frac{P(A, D)}{P(A)P(D)}$$

By multiplying by $\frac{P(A)}{P(A)}$, the team calls out that we can reinterpret the problem as solving for a posterior probability $P(A\vert D)$ based on the prior probability $P(A)$ times the "information component" between the adverse event and the drug. This concept is borrowed from information theory, but ultimately boils down to the team solving for the following metric:

$$IC=\log_2\frac{P(A,D)}{P(A)P(D)}$$

<details markdown="1">
<summary>Click here for some brief comments on information theory.</summary>
<br>
In information theory, the information component is derived from mutual information ($I$), which is defined as:

$$I(X, Y)=\Sigma_x\Sigma_yP(x,y)\log{\frac{P(x,y)}{P(x)P(y)}}$$

The fraction contained in the summation of the mutual information equation is known as the **information component**, and exists for all potential values $x, y$. We are only interested in when the adverse event of interest occurs (A) and when the drug is present (D). So we only consider one information component, which we define as:

$$IC = \log_2{\frac{P(A, D)}{P(A)P(D)}}$$

Note that if A and D are independent, then $P(A, D)=P(A)P(D)$ and we the information component would be 0. Therefore, we can restate the analysis as $\frac{\text{Observed}}{\text{Expected}}$, since our initial expectation is that this independence holds (Assumption #1).
</details>
<br>

Suppose we have data that allow us to infer $P(A\vert D)$. Given a set of $n$ drug reports, our drug of interest appears $n_D$ times, the adverse event appears $n_A$ times, and the drug and adverse event appear together $n_{AD}$ times. This can be visualized below:

| | Event | Not event | Total |
| --- | --- | --- | --- |
| Drug | $n_{AD}$ | - | $n_D$ |
| Not drug | - | - | - |
| **Total** | $n_A$ | - | $n$ |

Without knowledge of the true probabilities $\{P(A), P(D), P(A,D)\}$, we set priors using beta distributions.

<details markdown="1">
<summary>Click here for a brief review of beta distributions as conjugate priors for Binomial distributions in Bayesian analysis.</summary>
<br>
*Information from [this educational video](https://www.youtube.com/watch?v=hKYvZF9wXkk).*

To gain some intuition into why the beta distribution is used here, we can consider a Bayesian formulation to approximate a distribution parameter $\theta$ for some data generating process that yields data $X\sim\text{Binomial}(n, \theta)$.

$$P(\theta\vert X)=\frac{P(X\vert\theta)P(\theta)}{P(X)}\propto P(X|\theta)P(\theta)$$

Assuming $\theta$ follows a beta distribution and $X$ follows a Binomial distribution, we know these probabilites:

$$\theta\sim\text{Beta}(\alpha=\alpha_0, \beta=\beta_0)=\frac{\theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}}{B(\alpha_0, \beta_0)}\rightarrow P(\theta)\propto \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1} \\
 \\
P(X=k\vert\theta)=\theta^k(1-\theta)^{n-k} \text{ for k-many successes in n trials}$$

Note the function $B$ in the beta distribution PDF is the beta function, which normalizes the output to constrain the interval from zero to one. We can think of this as a constant, which we ignore for now. Now we can substitute the solutions above into the original equation:

$$P(\theta\vert X)\propto \theta^{k+\alpha_0-1}(1-\theta)^{n+\beta_0-k-1}\rightarrow P(\theta\vert X)\sim \text{Beta}(\alpha=k+\alpha_0,\beta=n+\beta_0-k)$$

Therefore, using a beta prior yields consistent results, i.e., both the prior and posterior probabilites ($P(\theta), P(\theta\vert X)$) follow a beta distribution. The process above also depicts how to apply a likelihood ($P(X\vert\theta)$) to update our prior, which is a form of iterative Bayesian estimation. This is used in the analytic formulation of the BCPNN.

There are several properties of beta distributions used in the application of the BCPNN model. Assuming $X\sim\text{Beta}(\alpha, \beta)$:

$$E[X]=\frac{\alpha}{\alpha+\beta} \\
E[\ln{(X)}]=\psi(\alpha)-\psi(\alpha+\beta) \\
\text{Var}[X]=\frac{\alpha+\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \\
\text{Var}[\ln{(X)}]=\psi_1(\alpha)-\psi_1(\alpha+\beta)$$

Note that $\psi$ is the digamma function and $\psi_1$ is the trigamma function.
</details>
<br>

The Bayesian framework is presented in the table below and integrates Assumption #2.

| Observed Data | Data Generation Assumption | Estimator | Initializations |
| --- | --- | --- | --- |
| Drug count ($n_D$) | $n_D\sim\text{Binomial}(n,p_D)$ | $p_D\sim \text{Beta}(\alpha_D, \beta_D)$ | $\alpha_{D0}=1,$ $\beta_{D0}=1$ |
| Adverse event count ($n_A$) | $n_A\sim\text{Binomial}(n,p_A)$ | $p_A\sim \text{Beta}(\alpha_A, \beta_A)$ | $\alpha_{A0}=1,$ $\beta_{A0}=1$ |
| Adverse event *and* drug count ($n_{AD}$) | $n_{AD}\sim\text{Binomial}(n,p_{AD})$ | $p_{AD}\sim \text{Beta}(\alpha_{AD}, \beta_{AD})$ | $\alpha_{AD0}=1,$ $\beta_{AD0}=\frac{1}{E[p_D]E[p_A]}-1$ |

The beta priors are initialized assuming the probability of the drug, $P(D)$, and the probability of the adverse event, $P(A)$, are entirely unknown such that the distributions of $p_D$ and $p_A$ are uniform across the interval [0, 1]. Note that the probability of the drug and adverse event occurring together reflects that we initially believe their occurrence to be independent, i.e., $E[p_{AD}]=E[p_D]E[p_A]$ (see Assumption #1).

## The Neural Network

To extrapolate the process above to all potential drug and adverse event combinations, the team proposed using a neural network. They used a one-layer model, which can be formed as a fully connected bipartite graph composed of one layer of drugs and one layer of adverse events. The edge weights in the network reflect the information component for that drug-adverse event pair.

The authors acknowledge the network could have multiple layers, but stick with a one-layer approach for the initial BCPNN implementation. This allows us to solve for closed for solutions for the weights (i.e., information components) in the network.

## The Closed-Form Solution(s)

The original 1998 paper [1] proposes using the expected values of the beta priors to plug into the information component: *"The expectation value of each beta distribution is the estimate of the probability."* We can first show the derivation for the prior and posterior probability estimates:

Prior Probability | Posterior Probability
--- | ---
$P(D)=E[p_D]=\frac{\alpha_{D0}}{\alpha_{D0}+\beta_{D0}}$ $=\frac{1}{2}$ | $E[p_D]=\frac{n_D+\alpha_{D0}}{n_D+\alpha_{D0}+n+\beta_{D0}-n_D}=\frac{n_D+1}{n+2}$
$P(A)=E[p_A]=\frac{\alpha_{A0}}{\alpha_{A0}+\beta_{A0}}$ $=\frac{1}{2}$ | $E[p_A]=\frac{n_A+\alpha_{A0}}{n_A+\alpha_{A0}+n+\beta_{A0}-n_A}$ $=\frac{n_A+1}{n+2}$
$P(A,D)=E[p_{AD}]=\frac{\alpha_{AD0}}{\alpha_{AD0}+\beta_{AD0}}$ $=\frac{1}{1/(E[p_A]E[p_D])-1}=\frac{1}{4}$ | $E[p_{AD}]=\frac{n_{AD}+\alpha_{AD0}}{n_{AD}+\alpha_{AD0}+n+\beta_{AD0}-n_{AD}}$ $=\frac{n_{AD}+1}{n+1/(E[p_A]E[p_D])}$

Now we can plug in the posterior probabilities to the information component:

$$\text{IC}=\log_2\frac{P(A,D)}{P(A)P(D)}=\log_2\frac{E[p_{AD}]}{E[p_A]E[p_D]}=\log_2\left[\frac{1}{E[p_A]E[p_D]}\left(\frac{n_{AD}+1}{n+1/(E[p_A]E[p_D])}\right)\right] \\
\rightarrow \hat{\text{IC}}=\log_2\frac{n_{AD}+1}{nE[p_A]E[p_D]+1}$$

At this stage, we can see that the information criteria is reflecting the $\left(\frac{\text{Observed}}{\text{Expected}}\right)$ problem statement originally stated in the theoretical framework. The "expected" in this case reflects the conservative assumption that the drug and adverse effect are independent (unassociated). Plugging in the other expectations (see table above) yields the following closed-form solution:

$$\hat{\text{IC}}=\log_2\frac{(n_{AD}+1)(n+2)^2}{n(n_A+1)(n_D+1)+(n+2)^2}$$

The variance of the information component was estimated using the "delta method" ([see here for a helpful R vignette](https://cran.r-project.org/web/packages/anovir/vignettes/confidence_intervals.html)) as an approximation.

<details markdown="1">
<summary>Click here for the Bates's variance derivation.</summary>
<br>
We are looking to solve for the variance of the information component:

$$\text{(1)}\quad\quad\text{Var}[\hat{\text{IC}}]=\text{Var}[ \log_2\frac{P(A,D)}{P(A)P(D)}] \\ =\text{Var}[ \log_2P(A,D)-\log_2P(A)-\log_2P(D)]$$

By the delta method, we can approximate the variance of a function with multiple random variables by leveraging first order derivatives:

$$\text{(2)}\quad\quad\text{Var}[g]\approx\text{Var}[f(X_1,X_2,...,X_n)] \\ =\Sigma_{i=1}^n\left(\frac{\delta f}{\delta X_i}\right)^2\text{Var}[X_i]+2\Sigma_{i=1}^n\Sigma_{j=1}^n\left(\frac{\delta f}{\delta X_i}\right)\left(\frac{\delta f}{\delta X_j}\right)\text{Cov}(X_i,X_j)$$

**Bates and team assume independence (zero covariance),** which may not be realistic but simplifies the derivation.

Using the derivative rule for logarithms, we can solve for the partial derivative of any probability in the information component. Note that the partials are not dependent on any other probabilities, which is depicted in Equation #1.

$$\text{(3)}\quad\quad\frac{\delta\hat{\text{IC}}}{\delta P(\cdot )}=\frac{1}{E[p]\ln(2)}=\frac{\alpha+\beta}{\alpha\ln(2)}$$

Recall that the variance for any $X\sim \text{Beta}(\alpha, \beta)$ is:

$$\text{(4)}\quad\quad\text{Var}[X]=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$$

We combine Equation #2 (dropping the covariance), Equation #3, and Equation #4 to produce:

$$\text{(5)}\quad\quad\text{Var}[\hat{\text{IC}}]\approx\Sigma_{i=1}^n\left(\frac{\delta f}{\delta X_i}\right)^2\text{Var}[X_i] \\
=\Sigma_{i=1}^n\left(\frac{\alpha+\beta}{\alpha\ln(2)}\right)^2\left(\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\right) \\
=\left(\frac{1}{\ln(2)}\right)^2\Sigma_{i=1}^n\left(\frac{\beta}{\alpha(\alpha+\beta+1)}\right)$$

The posterior alpha and betas for the 3 probabilities $\{P(A,D),P(A),P(D)\}$ are presented below:

Posterior Alpha | Posterior Beta | Poster Alpha + Posterior Beta
--- | --- | ---
$\alpha_D=n_D+\alpha_{D0}=n_D+1$ | $\beta_D=n+\beta_{D0}-n_D$ $=n+1-n_D$ | $\alpha_D+\beta_D=n+2$
$\alpha_A=n_A+\alpha_{A0}=n_A+1$ | $\beta_A=n+\beta_{A0}-n_A$ $=n+1-n_A$ | $\alpha_A+\beta_A=n+2$
$\alpha_{AD}=n_{AD}+\alpha_{AD0}=n_{AD}+1$ | $\beta_{AD}=n+\beta_{AD0}-n_{AD}$ $=n+\frac{1}{E[p_A]E[p_D]}-1-n_{AD}$ | $\alpha_{AD}+\beta_{AD}$ $=n+\frac{(n+2)^2}{(n_A+1)(n_D+1)}$

Plugging these values into Equation #5 yields the final solution:

$$\text{Var}[\hat{\text{IC}}]\approx\left(\frac{1}{\ln(2)}\right)^2\left(\frac{n+\gamma-1-n_{AD}}{(n_{AD}+1)(n+\gamma+1)}+\frac{n+1-n_A}{(n_A+1)(n+2)}+\frac{n+1-n_D}{(n_D+1)(n+2)}\right), \\ \gamma=\frac{(n+2)^2}{(n_A+1)(n_D+1)}$$

</details>
<br>

In 2002, Gould publishes a paper presenting the solutions for the exact expected value and variance of the information component [3]. This derivation is currently implemented in open-source R packages, such as the [`pvm` (Pharmacovigilance Methods) package](https://rdrr.io/github/bips-hb/pvm/src/R/BCPNN.R).

<details markdown="1">
<summary>Click here for the alternative derivation.</summary>
<br>
Instead of setting $P(A)=E[p_A]$ (etc.), we can solve for the expected value of the information component using the random variables $p_A$ (etc.). Using the change of base formula and properties of expected value, we find:

$$E[\hat{\text{IC}}]=E\left[\log_2\left(\frac{p_{AD}}{p_Ap_D}\right)\right]=\frac{1}{\ln(2)}\left(E[\ln p_{AD}]-E[\ln p_A]-E[\ln p_D]\right)$$

Recall that for random variables $X\sim\text{Beta}(\alpha,\beta)\rightarrow E[\ln(X)]=\psi(\alpha)-\psi(\alpha+\beta)$, where $\psi$ is the [digamma function](https://handwiki.org/wiki/Digamma_function). Therefore, we have:

$$\frac{1}{\ln(2)}\left( E[\ln p_{AD}]-E[\ln p_A]-E[\ln p_D]\right) \\
=\frac{1}{\ln(2)}\left[\psi(\alpha_{AD})-\psi(\alpha_{AD}+\beta_{AD})-\psi(\alpha_A)+\psi(\alpha_A+\beta_A)-\psi(\alpha_D)+\psi(\alpha_D+\beta_D)\right]$$

We can plug in the posterior distribution parameters (see table in Bates's variance derivation above). This yields a final analytic solution:

$$E[\hat{\text{IC}}]=\frac{1}{\ln(2)}\left[\psi(\alpha_{AD})-\psi(\alpha_{AD}+\beta_{AD})-\psi(\alpha_A)+\psi(\alpha_A+\beta_A)-\psi(\alpha_D)+\psi(\alpha_D+\beta_D)\right] \\ =\frac{1}{\ln(2)}\left[\psi(n_{AD}+1)-\psi\left(n+\frac{(n+2)^2}{(n_A+1)(n_D+1)}\right)-\psi(n_A+1)+\psi(n+2)-\psi(n_D+1)+\psi(n+2)\right]$$

We can perform the same operations for the variance of $\hat{\text{IC}}$. Keep in mind that, assuming independence, $\text{Var}[aX-Y]=a^2\text{Var}[X]+(-1)^2\text{Var}[Y]=a^2\text{Var}[X]+\text{Var}[Y]$. Additionally, for any random variable $X\sim\text{Beta}(\alpha,\beta)\rightarrow\text{Var}[\ln(X)]=\psi_1(\alpha)-\psi_1(\alpha+\beta)$, where $\psi_1$ is the [trigamma function](https://handwiki.org/wiki/Trigamma_function).

$$\text{Var}[\hat{\text{IC}}]=\frac{1}{\ln(2)^2}\left[\psi_1(\alpha_{AD})-\psi_1(\alpha_{AD}+\beta_{AD})+\psi_1(\alpha_A)-\psi_1(\alpha_A+\beta_A)+\psi(\alpha_D)-\psi_1(\alpha_D+\beta_D)\right] \\ =\frac{1}{\ln(2)}\left[\psi_1(n_{AD}+1)-\psi_1\left(n+\frac{(n+2)^2}{(n_A+1)(n_D+1)}\right)+\psi_1(n_A+1)-\psi_1(n+2)+\psi_1(n_D+1)-\psi_1(n+2)\right]$$

<b>Note that this assumes independence</b>, since the covariance is assumed to be zero.
</details>
<br>

Both closed form solutions require sufficiently large sample sizes, since they rely on the normally-distributed expectations for the final confidence intervals. For small samples, Noren and team proposed a Monte Carlo simulation approach [4] that has been widely adopted.

# Citations

1. Bate, Andrew, et al. "A Bayesian neural network method for adverse drug reaction signal generation." *European journal of clinical pharmacology* 1998; 54.4: 315-321.
2. DuMouchel, William, Pregibon, Daryl. "Empirical bayes screening for multi-item associations." *Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining* 2001.
3. Lawrence Gould, A. "Practical pharmacovigilance analysis strategies." *Pharmacoepidemiology and drug safety* 2003; 12.7: 559-574.
4. Noren, GN, Bate, A, Orre, R, Edwards, IR. "Extending the methods used to screen the who drug safety database towards analysis of complex associations and improved accuracy for rare events." *Statistics in Medicine* 2006; 25(21):3740–3757.

These additional sources assisted in deriving the closed form solutions:

- Ahmed, Ismaïl, et al. "Bayesian pharmacovigilance signal detection methods revisited in a multiple comparison setting." *Statistics in medicine* 2009; 28.13: 1774-1792.
- Xiao, Nan, et al. "Base Ranker: Bayesian Confidence Propagation Neural Network" 2020; https://nanx.me/rankv/analysis-bcpnn.html.
