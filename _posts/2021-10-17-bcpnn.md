---
title: "Pharmacovigilance Methods: BCPNNs"
layout: post
---

<!-- https://stackoverflow.com/a/46765337 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']]
        }
    })
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<style>
/* Hide the toggle checkbox */
details {
    box-shadow: 2px 2px 5px 2px black;
}

details > summary {
    font-weight: bold;
    align-items: left;
    vertical-align: middle;
    padding: 2px 6px;
    cursor: pointer;
}

details > p {
    padding: 2px 6px;
    margin: none;
}
</style>

Pharmacovigilance is the field of science that monitors, detects, and prevents adverse drug reactions (ADRs). While monitoring and prevention are critical components in drug safety, we will focus on "signal detection." Signal detection in drug surveillance is primarily an analytics problem that needs to account for the complexities in healthcare data and (mostly) passive data collection.

In 1998, the Bayesian Confidence Propagation Neural Network (BCPNN) method was introduced to the field in a publication by Bates and team [1]. The team used an information theory framework, which takes the *signal* detection task quite literally since information theory focuses on quantifying signals within communication streams. The team then applies Bayesian inference to detect signals in the data.

## The Idea

The idea revolves around inferring the probability of an adverse drug event (A) given some drug of interest (D). To solve for this probability, $P(A\vert D)$, the researchers make the following assumptions:

1. **Null Hypothesis of Independence:** There are thousands of potential adverse events for any drug, and many of those events (if not all) are not associated with taking specific drugs. Therefore, the team asserts a prior assumption that $P(A)\perp\kern-5pt\perp P(D)$.
2. **Binomially-Distributed Data:** With respect to the observed data (e.g., spontaneous reporting system), the team asserts that the counts of (i) the drug of interest, (ii) the adverse event of interest, and (iii) the drug of interest and adverse event of interest co-occurring all follow a Binomial distribution.

The BCPNN method tests the initial assumption of independence through statistical inference.

## The Theory

As a quick summary of the 1998 paper [1], the team applies Bayes' law:

$$P(A\vert D) = \frac{P(A, D)}{P(D)} = P(A)\frac{P(A, D)}{P(A)P(D)}$$

By multiplying by $\frac{P(A)}{P(A)}$, the team calls out that we can reinterpret the problem as solving for a posterior probability $P(A\vert D)$ based on the prior probability $P(A)$ times the "information component" between the ADR and the drug. This is borrowed from information theory, but ultimately boils down to the team solving for the following metric using BCPNNs:

$$IC=\log_2\frac{P(A,D)}{P(A)P(D)}$$

<details markdown="1">
<summary>Click here for some brief comments on information theory.</summary>
<br>
In information theory, the information component is derived from mutual information ($I$) which is defined as:

$$I(X, Y)=\Sigma_x\Sigma_yP(x,y)\log{\frac{P(x,y)}{P(x)P(y)}}$$

The fraction contained in the summation of the mutual information equation is known as the **information component**, and exists for all potential values $x, y$. We are only interested in when the adverse event of interest occurs (A) and when the drug is present (D). So we only consider one information component, which we define as:

$$IC = \log_2{\frac{P(A, D)}{P(A)P(D)}}$$

Note that if A and D are independent, then $P(A, D)=P(A)P(D)$ and we the information component would be 0. Therefore, we can restate the analysis as $\frac{\text{Observed}}{\text{Expected}}$, since our null hypothesis (initial expectation) is that this independence holds (Assumption #1).
</details>
<br>

Suppose we have data that allow us to infer $P(A\vert D)$. Given a set of $n$ drug reports, our drug of interest appears $n_D$ times, the adverse event appears $n_A$ times, and the drug and adverse event appear together $n_{AD}$ times. This can be visualized below:

 | Event | Not event | Total
--- | --- | --- | ---
Drug | $n_{AD}$ | - | $n_D$
Not drug | - | - | -
**Total** | $n_A$ | - | $n$

Without knowledge of the true probabilities $\{P(A), P(D), P(A,D)\}$, we set priors using beta distributions.

<details markdown="1">
<summary>Click here for a brief review of beta distributions as conjugate priors for Binomial distributions in Bayesian analysis.</summary>
<br>
*Information from [this educational video](https://www.youtube.com/watch?v=hKYvZF9wXkk).*

To gain some intuition into why the beta distribution is used here, we can consider a Bayesian formulation to approximate a distribution parameter $\theta$ for some data generating process that yields data $X\sim\text{Binomial}(n, \theta)$.

$$P(\theta\vert X)=\frac{P(X\vert\theta)P(\theta)}{P(X)}\propto P(X|\theta)P(\theta)$$

Assuming $\theta$ follows a beta distribution and $X$ follows a Binomial distribution, we know these probabilites:

$$\theta\sim\text{Beta}(\alpha=\alpha_0, \beta=\beta_0)=\frac{\theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}}{B(\alpha_0, \beta_0)}\rightarrow P(\theta)\propto \theta^{\alpha_0-1}(1-\theta)^{\beta_0-1} \\
 \\
P(X=k\vert\theta)=\theta^k(1-\theta)^{n-k} \text{ for k-many successes in n trials}$$

Note the function $B$ in the beta distribution PDF is the beta function, which normalizes the output to constrain the interval from zero to one. We can think of this as a constant, which we ignore for now. Now we can substitute the solutions above into the original equation:

$$P(\theta\vert X)\propto \theta^{k+\alpha_0-1}(1-\theta)^{n+\beta_0-k-1}\rightarrow P(\theta\vert X)\sim \text{Beta}(\alpha=k+\alpha_0,\beta=n+\beta_0-k)$$

Therefore, using a beta prior yields consistent results, i.e., both the prior and posterior probabilites ($P(\theta), P(\theta\vert X)$) follow a beta distribution. The process above also depicts how to apply a likelihood ($P(X\vert\theta)$) to update our prior, which is a form of iterative Bayesian estimation. This is used in the analytic formulation of the BCPNN.

There are several properties of beta distributions used in the application of the BCPNN model. Assuming $X\sim\text{Beta}(\alpha, \beta)$:

$$E[X]=\frac{\alpha}{\alpha+\beta} \\
E[\ln{(X)}]=\psi(\alpha)-\psi(\alpha+\beta) \\
\text{Var}[X]=\frac{\alpha+\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \\
\text{Var}[\ln{(X)}]=\psi_1(\alpha)-\psi_1(\alpha+\beta)$$

Note that $\psi$ is the digamma function and $\psi_1$ is the trigamma function.
</details>
<br>

The Bayesian framework is presented in the table below.

Observed Data | Assumption | Estimator | Initializations
--- | --- | --- | ---
*Data from drug reports* | *Belief of how data was generated* | *Estimation of data generating process* | *Initial etimator parameters*
Drug count ($n_D$) | $n_D\sim\text{Binomial}(n,p_D)$ | $p_D\sim \text{Beta}(\alpha_D, \beta_D)$ | $\alpha_{D0}=1,$ $\beta_{D0}=1$
Adverse event count ($n_A$) | $n_A\sim\text{Binomial}(n,p_A)$ | $p_A\sim \text{Beta}(\alpha_A, \beta_A)$ | $\alpha_{A0}=1,$ $\beta_{A0}=1$
Adverse event *and* drug count ($n_{AD}$) | $n_{AD}\sim\text{Binomial}(n,p_{AD})$ | $p_{AD}\sim \text{Beta}(\alpha_{AD}, \beta_{AD})$ | $\alpha_{AD0}=1,$ $\beta_{AD0}=\frac{1}{E[p_D]E[p_A]}-1$

The beta priors are initialized assuming the probability of the drug, $P(D)$, and the probability of the adverse event, $P(A)$, are entirely unknown such that the distributions of $p_D$ and $p_A$ are uniform across the interval [0, 1]. Note that the probability of the drug and adverse event occurring together reflects that we initially believe their occurrence to be independent, i.e., $P(A, D)=P(A)P(D)$ and $E[p_{AD}]=E[p_D]E[p_A]$.

## The Closed-Form Solution(s)

The original 1998 paper [1] proposes using the expected values of the beta priors to plug into the information component. We can first show the derivation for the prior and posterior probability estimates:

Prior Probability | Posterior Probability
--- | ---
$P(D)=E[p_i]=\frac{\alpha_{i0}}{\alpha_{i0}+\beta_{i0}}$ $=\frac{1}{2}$ | $E[p_i\vert n_i]=\frac{n_i+\alpha_{i0}}{n_i+\alpha_{i0}+n+\beta_{i0}-n_i}=\frac{n_i+1}{n+2}$
$P(A)=E[p_j]=\frac{\alpha_{j0}}{\alpha_{j0}+\beta_{j0}}$ $=\frac{1}{2}$ | $E[p_j\vert n_j]=\frac{n_j+\alpha_{j0}}{n_j+\alpha_{j0}+n+\beta_{j0}-n_j}$ $=\frac{n_j+1}{n+2}$
$P(A,D)=E[p_{ij}]=\frac{\alpha_{ij0}}{\alpha_{ij0}+\beta_{ij0}}$ $=\frac{1}{1/(E[p_i]E[p_j])-1}=\frac{1}{4}$ | $E[p_{ij}\vert n_{ij}]=\frac{n_{ij}+\alpha_{ij0}}{n_{ij}+\alpha_{ij0}+n+\beta_{ij0}-n_{ij}}$ $=\frac{n_{ij}+1}{n+1/(E[p_i]E[p_j])}$

Now we can plug in the posterior probabilities to the information component:

$$\text{IC}=\log_2\frac{P(A,D)}{P(A)P(D)}=\log_2\frac{E[p_{ij}]}{E[p_i]E[p_j]}=\log_2\left[\frac{1}{E[p_i]E[p_j]}\left(\frac{n_{ij}+1}{n+1/(E[p_i]E[p_j])}\right)\right] \\
\rightarrow \hat{\text{IC}}=\log_2\frac{n_{ij}+1}{nE[p_i]E[p_j]+1}$$

At this stage, we can see that the information criteria is reflecting the $\left(\frac{\text{Observed}}{\text{Expected}}\right)$ problem statement that we originally desired in the theoretical framework. The "expected" in this case reflects the conservative assumption that the drug and adverse effect are independent (unassociated). Plugging in the rest of the values yields the following closed-form solution:

$$\hat{\text{IC}}=\log_2\frac{(n_{ij}+1)(n+2)^2}{n(n_i+1)(n_j+1)+(n+2)^2}$$

In 2002, Gould publishes a paper presenting the solutions for the expected value and variance of the information component without using the expected value of each probability [2]. This derivation is currently implemented in open-source R packages, such as [`pvm` R package](https://rdrr.io/github/bips-hb/pvm/src/R/BCPNN.R).

<details markdown="1">
<summary>Click here for the alternative derivation.</summary>
<br>
Instead of setting $P(A)=E[p_i]$ (etc.), we can solve for the expected value of the information component using the random variables $p_i$ (etc.). Using the change of base formula and properties of expected value, we find:

$$E[\hat{\text{IC}}]=E\left[\log_2\left(\frac{p_{ij}}{p_ip_j}\right)\right]=E\left[\frac{1}{\ln(2)}\ln\left(\frac{p_{ij}}{p_ip_j}\right)\right]=\frac{1}{\ln(2)}E\left[\ln\left(\frac{p_{ij}}{p_ip_j}\right)\right] \\ =\frac{1}{\ln(2)}E\left[\ln p_{ij}-\ln p_i-\ln p_j\right]=\frac{1}{\ln(2)}\left(E[\ln p_{ij}]-E[\ln p_i]-E[\ln p_j]\right)$$

Recall that for random variables $X\sim\text{Beta}(\alpha,\beta)\rightarrow E[\ln(X)]=\psi(\alpha)-\psi(\alpha+\beta)$, where $\psi$ is the [digamma function](https://handwiki.org/wiki/Digamma_function). Therefore, we have:

$$\frac{1}{\ln(2)}\left( E[\ln p_{ij}]-E[\ln p_i]-E[\ln p_j]\right) \\
=\frac{1}{\ln(2)}\left[\psi(\alpha_{ij})-\psi(\alpha_{ij}+\beta{ij})-\psi(\alpha_i)+\psi(\alpha_i+\beta_i)-\psi(\alpha_j)+\psi(\alpha_j+\beta_j)\right]$$

We can plug in the posterior distribution parameters, which are summarized in the table below.

Posterior Alpha | Posterior Beta | Poster Alpha + Posterior Beta
--- | --- | ---
$\alpha_i=n_i+\alpha_{i0}=n_i+1$ | $\beta_i=n+\beta_{i0}-n_i=n+1-n_i$ | $\alpha_i+\beta_i=n+2$
$\alpha_j=n_j+\alpha_{j0}=n_j+1$ | $\beta_j=n+\beta_{j0}-n_j=n+1-n_j$ | $\alpha_j+\beta_j=n+2$
$\alpha_{ij}=n_{ij}+\alpha_{ij0}=n_{ij}+1$ | $\beta_{ij}=n+\beta_{ij0}-n_{ij}$ $=n+\frac{1}{E[p_i]E[p_j]}-1-n_{ij}$ | $\alpha_{ij}+\beta_{ij}=n+\frac{(n+2)^2}{(n_i+1)(n_j+1)}$

This yields a final analytic solution:

$$E[\hat{\text{IC}}]=\frac{1}{\ln(2)}\left[\psi(\alpha_{ij})-\psi(\alpha_{ij}+\beta{ij})-\psi(\alpha_i)+\psi(\alpha_i+\beta_i)-\psi(\alpha_j)+\psi(\alpha_j+\beta_j)\right] \\ =\frac{1}{\ln(2)}\left[\psi(n_{ij}+1)-\psi\left(n+\frac{(n+2)^2}{(n_i+1)(n_j+2)}\right)-\psi(n_i+1)+\psi(n+2)-\psi(n_j+1)+\psi(n+2)\right]$$

We can perform the same operations for the variance of $\hat{\text{IC}}$. Keep in mind that, assuming independence, $\text{Var}[aX-Y]=a^2\text{Var}[X]+(-1)^2\text{Var}[Y]=a^2\text{Var}[X]+\text{Var}[Y]$. Additionally, for any random variable $X\sim\text{Beta}(\alpha,\beta)\rightarrow\text{Var}[\ln(X)]=\psi_1(\alpha)-\psi_1(\alpha+\beta)$, where $\psi_1$ is the [trigamma function](https://handwiki.org/wiki/Trigamma_function).

$$\text{Var}[\hat{\text{IC}}]=\frac{1}{\ln(2)^2}\left[\psi_1(\alpha_{ij})-\psi_1(\alpha_{ij}+\beta{ij})+\psi_1(\alpha_i)-\psi_1(\alpha_i+\beta_i)+\psi(\alpha_j)-\psi_1(\alpha_j+\beta_j)\right] \\ =\frac{1}{\ln(2)}\left[\psi_1(n_{ij}+1)-\psi_1\left(n+\frac{(n+2)^2}{(n_i+1)(n_j+2)}\right)+\psi_1(n_i+1)-\psi_1(n+2)+\psi_1(n_j+1)-\psi_1(n+2)\right]$$

<b>Note that this assumes independence</b>, since the covariance is assumed to be zero.
</details>
<br>

# Citations

1. A Bayesian neural network method for adverse drug reaction signal generation
2. Practical pharmacovigilance analysis strategies
3. Bayesian pharmacovigilance signal detection methods revisited in a multiple comparison setting
4. https://nanx.me/rankv/analysis-bcpnn.html